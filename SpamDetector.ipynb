{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbLIPwsQN8De5IddcwwNqc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S4vyss/machine-learning/blob/main/SpamDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
        "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
        "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
        "SPAM_PATH = os.path.join(\"zestawy danych\", \"spam\")\n",
        "\n",
        "def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
        "    if not os.path.isdir(spam_path):\n",
        "        os.makedirs(spam_path)\n",
        "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n",
        "        path = os.path.join(spam_path, filename)\n",
        "        if not os.path.isfile(path):\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "        tar_bz2_file = tarfile.open(path)\n",
        "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
        "        tar_bz2_file.close()"
      ],
      "metadata": {
        "id": "aTOwEAno678I"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_spam_data()"
      ],
      "metadata": {
        "id": "8UApu0BfBCgz"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
        "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
        "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
        "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
      ],
      "metadata": {
        "id": "ArMXKKi3C4hY"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import email\n",
        "import email.policy\n",
        "\n",
        "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
        "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
        "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
        "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
      ],
      "metadata": {
        "id": "Hpu6T9wiDBIJ"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
        "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
      ],
      "metadata": {
        "id": "8dvmzjd7DH92"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ham_emails[0].get_content().strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP-8EDH2DMWi",
        "outputId": "5e6f23fb-79fb-4d9a-8165-193c50745a29"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date:        Wed, 21 Aug 2002 10:54:46 -0500\n",
            "    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
            "    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
            "\n",
            "\n",
            "  | I can't reproduce this error.\n",
            "\n",
            "For me it is very repeatable... (like every time, without fail).\n",
            "\n",
            "This is the debug log of the pick happening ...\n",
            "\n",
            "18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\n",
            "18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\n",
            "18:19:04 Ftoc_PickMsgs {{1 hit}}\n",
            "18:19:04 Marking 1 hits\n",
            "18:19:04 tkerror: syntax error in expression \"int ...\n",
            "\n",
            "Note, if I run the pick command by hand ...\n",
            "\n",
            "delta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\n",
            "1 hit\n",
            "\n",
            "That's where the \"1 hit\" comes from (obviously).  The version of nmh I'm\n",
            "using is ...\n",
            "\n",
            "delta$ pick -version\n",
            "pick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\n",
            "\n",
            "And the relevant part of my .mh_profile ...\n",
            "\n",
            "delta$ mhparam pick\n",
            "-seq sel -list\n",
            "\n",
            "\n",
            "Since the pick command works, the sequence (actually, both of them, the\n",
            "one that's explicit on the command line, from the search popup, and the\n",
            "one that comes from .mh_profile) do get created.\n",
            "\n",
            "kre\n",
            "\n",
            "ps: this is still using the version of the code form a day ago, I haven't\n",
            "been able to reach the cvs repository today (local routing issue I think).\n",
            "\n",
            "\n",
            "\n",
            "_______________________________________________\n",
            "Exmh-workers mailing list\n",
            "Exmh-workers@redhat.com\n",
            "https://listman.redhat.com/mailman/listinfo/exmh-workers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "VuXsU3GzO45V"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(ham_emails + spam_emails)\n",
        "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))"
      ],
      "metadata": {
        "id": "hfuTsPHmBFbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82df1c5f-6b07-494e-ca83-222eeacab2ba"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-199-2e26613a2d67>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array(ham_emails + spam_emails)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "gLc0miBTXxJC"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from html import unescape\n",
        "\n",
        "def html_to_plain_text(html):\n",
        "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
        "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
        "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
        "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
        "    return unescape(text)"
      ],
      "metadata": {
        "id": "DT2TKH2AM0YJ"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def email_to_text(email):\n",
        "    html = None\n",
        "    for part in email.walk():\n",
        "        ctype = part.get_content_type()\n",
        "        if not ctype in (\"text/plain\", \"text/html\"):\n",
        "            continue\n",
        "        try:\n",
        "            content = part.get_content()\n",
        "        except: # w przypadku problemów z kodowaniem\n",
        "            content = str(part.get_payload())\n",
        "        if ctype == \"text/plain\":\n",
        "            return content\n",
        "        else:\n",
        "            html = content\n",
        "    if html:\n",
        "        return html_to_plain_text(html)"
      ],
      "metadata": {
        "id": "qwQ07j02M8XV"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import nltk\n",
        "\n",
        "    stemmer = nltk.PorterStemmer()\n",
        "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
        "        print(word, \"=>\", stemmer.stem(word))\n",
        "except ImportError:\n",
        "    print(\"Błąd: proces analizy słowotwórczej wymaga modułu NLTK.\")\n",
        "    stemmer = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5ooURhoNXMM",
        "outputId": "4adae3d4-3ba1-4d4e-822b-ddc8be66c6a2"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computations => comput\n",
            "Computation => comput\n",
            "Computing => comput\n",
            "Computed => comput\n",
            "Compute => comput\n",
            "Compulsive => compuls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# jeżeli przetwarzasz ten notatnik w środowisku Colab, korzystamy jedtnie z pip install urlextract\n",
        "try:\n",
        "    import google.colab\n",
        "    !pip install -q -U urlextract\n",
        "except ImportError:\n",
        "    pass # not running on Colab"
      ],
      "metadata": {
        "id": "Te5ijYjYNjDQ"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import urlextract # może wymagać połączenia internetowego w celu pobrania głównych nazw domenowych\n",
        "\n",
        "    url_extractor = urlextract.URLExtract()\n",
        "    print(url_extractor.find_urls(\"Czy wykryje github.com i https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
        "except ImportError:\n",
        "    print(\"Błąd: zastępowanie adresów URL wymaga obecności modułu urlextract.\")\n",
        "    url_extractor = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxgYJeDLNd-f",
        "outputId": "f188b588-764c-4770-eedf-8a40880ac7ab"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from collections import Counter\n",
        "\n",
        "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
        "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
        "        self.strip_headers = strip_headers\n",
        "        self.lower_case = lower_case\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.replace_urls = replace_urls\n",
        "        self.replace_numbers = replace_numbers\n",
        "        self.stemming = stemming\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for email in X:\n",
        "            text = email_to_text(email) or \"\"\n",
        "            if self.lower_case:\n",
        "                text = text.lower()\n",
        "            if self.replace_urls and url_extractor is not None:\n",
        "                urls = list(set(url_extractor.find_urls(text)))\n",
        "                urls.sort(key=lambda url: len(url), reverse=True)\n",
        "                for url in urls:\n",
        "                    text = text.replace(url, \" URL \")\n",
        "            if self.replace_numbers:\n",
        "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
        "            if self.remove_punctuation:\n",
        "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
        "            word_counts = Counter(text.split())\n",
        "            if self.stemming and stemmer is not None:\n",
        "                stemmed_word_counts = Counter()\n",
        "                for word, count in word_counts.items():\n",
        "                    stemmed_word = stemmer.stem(word)\n",
        "                    stemmed_word_counts[stemmed_word] += count\n",
        "                word_counts = stemmed_word_counts\n",
        "            X_transformed.append(word_counts)\n",
        "        return np.array(X_transformed)"
      ],
      "metadata": {
        "id": "YVvjWNCsMzAo"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vocabulary_size=1000):\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "    def fit(self, X, y=None):\n",
        "        total_count = Counter()\n",
        "        for word_count in X:\n",
        "            for word, count in word_count.items():\n",
        "                total_count[word] += min(count, 10)\n",
        "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
        "        self.most_common_ = most_common\n",
        "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for row, word_count in enumerate(X):\n",
        "            for word, count in word_count.items():\n",
        "                rows.append(row)\n",
        "                cols.append(self.vocabulary_.get(word, 0))\n",
        "                data.append(count)\n",
        "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
      ],
      "metadata": {
        "id": "LuH8DCxYNGHS"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "preprocess_pipeline = Pipeline([\n",
        "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
        "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
        "])\n",
        "\n",
        "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "Bbmk_V-BNwaT"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
        "score.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul5QJ3dgQGFq",
        "outputId": "70221b89-b3ba-441f-9900-f31a1d18f31f"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.981) total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.977) total time=   0.5s\n",
            "[CV] END ................................ score: (test=0.989) total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9823809523809524"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0paGY5fQmon"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}