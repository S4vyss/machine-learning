{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-28T17:29:31.400378Z","iopub.execute_input":"2023-09-28T17:29:31.400830Z","iopub.status.idle":"2023-09-28T17:29:31.408160Z","shell.execute_reply.started":"2023-09-28T17:29:31.400795Z","shell.execute_reply":"2023-09-28T17:29:31.406613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nclass MyLayerNormalization(keras.layers.Layer):\n    def __init__(self, eps=0.001, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        \n    def build(self, batch_input_shape):\n        self.alpha = self.add_weight(\n            name='alfa',\n            shape=(batch_input_shape[-1], ),\n            initializer='ones',\n            dtype=tf.float32,\n            trainable=True\n        )\n        self.beta = self.add_weight(\n            name='beta',\n            shape=(batch_input_shape[-1], ),\n            initializer='zeros',\n            dtype=tf.float32,\n            trainable=True\n        )\n        super().build(batch_input_shape)\n        \n    def call(self, X):\n        μ, σ = tf.nn.moments(X, axes=-1, keepdims=True)\n        std = tf.sqrt(σ)\n        return self.alpha * (X - μ) / (std + self.eps) + self.beta\n    \n    def compute_output_shape(self, batch_input_shape):\n        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n    \n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"eps\": self.eps}","metadata":{"execution":{"iopub.status.busy":"2023-09-28T17:29:31.410765Z","iopub.execute_input":"2023-09-28T17:29:31.411206Z","iopub.status.idle":"2023-09-28T17:29:31.429283Z","shell.execute_reply.started":"2023-09-28T17:29:31.411173Z","shell.execute_reply":"2023-09-28T17:29:31.427721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import LayerNormalization\n\ninput_data = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=np.float32)\n\nmy_layer = MyLayerNormalization(eps=1e-6)\nlayer_norm = LayerNormalization(epsilon=1e-6)\n\noutput_my_layer = my_layer(input_data)\noutput_layer_norm = layer_norm(input_data)\n\nif np.allclose(output_my_layer, output_layer_norm, rtol=1e-5, atol=1e-6):\n    print(\"Custom layer and LayerNormalization provide the same output.\")\nelse:\n    print(\"Custom layer and LayerNormalization provide different output.\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T17:29:31.431036Z","iopub.execute_input":"2023-09-28T17:29:31.432057Z","iopub.status.idle":"2023-09-28T17:29:31.461579Z","shell.execute_reply.started":"2023-09-28T17:29:31.431956Z","shell.execute_reply":"2023-09-28T17:29:31.460414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fashin_mnist = keras.datasets.fashion_mnist\n\n(X_train_full, y_train_full), (X_test, y_test) = fashin_mnist.load_data()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T17:40:15.989163Z","iopub.execute_input":"2023-09-28T17:40:15.989615Z","iopub.status.idle":"2023-09-28T17:40:20.913556Z","shell.execute_reply.started":"2023-09-28T17:40:15.989583Z","shell.execute_reply":"2023-09-28T17:40:20.912442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]","metadata":{"execution":{"iopub.status.busy":"2023-09-28T17:47:54.447158Z","iopub.execute_input":"2023-09-28T17:47:54.447641Z","iopub.status.idle":"2023-09-28T17:47:54.641797Z","shell.execute_reply.started":"2023-09-28T17:47:54.447610Z","shell.execute_reply":"2023-09-28T17:47:54.640549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nmodel.add(keras.layers.Dense(300, activation='relu'))\nmodel.add(keras.layers.Dense(100, activation='relu'))\nmodel.add(keras.layers.Dense(10, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T17:50:49.166923Z","iopub.execute_input":"2023-09-28T17:50:49.167493Z","iopub.status.idle":"2023-09-28T17:50:49.255524Z","shell.execute_reply.started":"2023-09-28T17:50:49.167449Z","shell.execute_reply":"2023-09-28T17:50:49.254352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_batch(X, y, batch_size=32):\n    idx = np.random.randint(len(X), size=batch_size)\n    return X[idx], y[idx]","metadata":{"execution":{"iopub.status.busy":"2023-09-28T17:55:48.922493Z","iopub.execute_input":"2023-09-28T17:55:48.922911Z","iopub.status.idle":"2023-09-28T17:55:48.928544Z","shell.execute_reply.started":"2023-09-28T17:55:48.922856Z","shell.execute_reply":"2023-09-28T17:55:48.927384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_status_bar(iteration, total, loss, metrics=None):\n    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss] + (metrics or [])])\n    end = \"\" if iteration < total else \"\\n\"\n    print(\"\\r{}/{}\".format(iteration, total) + metrics, end=end)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T18:03:18.332908Z","iopub.execute_input":"2023-09-28T18:03:18.333293Z","iopub.status.idle":"2023-09-28T18:03:18.339329Z","shell.execute_reply.started":"2023-09-28T18:03:18.333263Z","shell.execute_reply":"2023-09-28T18:03:18.338481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 10\nbatch_size = 32\nn_steps = len(X_train) // batch_size\noptimizer = keras.optimizers.Nadam(lr=0.01)\nloss_fn = keras.losses.sparse_categorical_crossentropy\nmean_loss = keras.metrics.Mean()\nmetrics = [keras.metrics.SparseCategoricalAccuracy()]","metadata":{"execution":{"iopub.status.busy":"2023-09-28T18:16:51.399596Z","iopub.execute_input":"2023-09-28T18:16:51.400034Z","iopub.status.idle":"2023-09-28T18:16:51.416445Z","shell.execute_reply.started":"2023-09-28T18:16:51.400001Z","shell.execute_reply":"2023-09-28T18:16:51.415321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1, n_epochs + 1):\n    print(\"Epoka numer {}/{}\".format(epoch, n_epochs))\n    for step in range(1, n_steps + 1):\n        X_batch, y_batch = random_batch(X_train, y_train)\n        with tf.GradientTape() as tape:\n            y_pred = model(X_batch, training=True)\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n            loss = tf.add_n([main_loss] + model.losses)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        mean_loss(loss)\n        for metric in metrics:\n            metric(y_batch, y_pred)\n        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n    for metric in [mean_loss] + metrics:\n        metric.reset_states()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T18:16:53.388495Z","iopub.execute_input":"2023-09-28T18:16:53.388881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}